# ğŸ’ƒğŸ”¥ MLFS â€“ Machine Learning Filesystem ğŸ’ƒğŸ”¥

*Mount your model like a USB stick and `cd` straight into its brain.*

---

## ğŸ¤”  Whatâ€theâ€heckâ€‘isâ€‘this?

MLFS is a tiny <abbr title="Filesystem in Userspace">FUSE</abbr> driver that turns a **PyTorch** or **ONNX** model into a directory tree:

```
/sys/          â† version, banner, full model string
/model/        â† every layer â†’ folder  â€¢   tensors â†’ files  â€¢   grads â†’ *.grad
/activations/  â† forwardâ€‘pass dumps
/ir/           â† TorchScript/ONNX graph
/logs/         â† tail â€‘f me plz
```

`ls`, `cat`, `diff`, `grep`â€¦ if your shell can touch it, it can now poke your network's guts.

---

## âš¡ï¸ 10â€‘second install

```bash
# Linux
apt install fuse              # or your distro's package manager
# macOS
brew install macfuse

# Python packages
pip install torch torchvision fusepy onnx
```

---

## ğŸ¸  Mount n' Roll

```bash
# Create a temporary mount point in your home directory
mkdir -p ~/mlfs_mount

# Export any .pt or .onnx you like (here we cook a ResNetâ€‘18)
python - <<'PY'
import torch, torchvision as tv
torch.save(tv.models.resnet18(weights=None), 'resnet18.pt')
PY

# Fire it up (foreground, ctrlâ€‘c to quit)
./mlfs.py --model resnet18.pt --mount ~/mlfs_mount --foreground
```

Now try the shellâ€‘fu:

```bash
ls ~/mlfs_mount/model/conv1
hexdump -C ~/mlfs_mount/model/conv1/weight.bin | head -n 4

echo "hello logğŸ‘‹" >> ~/mlfs_mount/logs/fuse.log
```

Unmount when done:

```bash
fusermount -u ~/mlfs_mount   # linux
# umount ~/mlfs_mount        # macOS
```

---

## ğŸ£  Example 1 â€” Tiny MLP *quick_demo.py*

```bash
cd mlfs/examples
python3 quick_demo.py
```

Watch it:

1. Builds a 2â€‘layer MLP on the fly.
2. Mounts MLFS in a temp dir.
3. Hexâ€‘spills the first 16 bytes of the weight matrix.
4. Patches one gradient byte (because we can âœ¨).
5. Cleans all traces like a ninja.

---

## ğŸ¦–  Example 2 â€” ResNetâ€‘18 *resnet_demo.py*

```bash
python3 examples/resnet_demo.py
```

## ğŸ•°ï¸ Gitâ€‘Timeâ€‘Machine Demo (ğŸš€ new!)
```bash
python examples/git_time_machine_demo.py
```
ğŸ•¸ï¸ MLFS mounts Inception v3 as regular files.
```
ğŸ“¸ git commit of /model = v1.
```
ğŸ”ª Flip one byte in a weight via the FS, commit again = v2.
```
git diff shows a â˜ï¸â€‘byte hex delta.
```
âª git checkout v1 rewinds the network instantlyâ€”no reloads, no downtime.

## ğŸ‰ Your neural net now responds to git log, git diff, and git checkout like any ordinary code repo.  Timeâ€‘travel debugging with zero custom tools!

## ğŸ¦Š Example 3 â€” ONNX Model *onnx_demo.py* (ğŸš€ new!)
```bash
python3 examples/onnx_demo.py
```
This demo:
1. Creates a simple neural network
2. Exports it to ONNX format
3. Mounts it using MLFS
4. Shows how to explore the model structure and weights

This heftier demo lists deep subâ€‘blocks (`layer1/0/conv1`) and dumps the raw conv1 tensor to prove MLFS handles real architectures.

---

## ğŸ› ï¸ 12 actuallyâ€‘useful things you can do (a totally serious list)

| #  | Useâ€‘case                                                                   | Emoji vibe |
| -- | -------------------------------------------------------------------------- | ---------- |
| 1  | **Unixâ€‘native surgery** â€“ `vim` a bias, save, watch trainer hotâ€‘reload     | ğŸ©º         |
| 2  | **Activation scooping** â€“ `inotifywait` + CSV dumps, zero extra hooks      | ğŸ¦         |
| 3  | **Timeâ€‘travel debugging** â€“ `git checkout stepâ€‘1337` and remount           | âŒ›ï¸         |
| 4  | **Quick viz** â€“ point TensorBoard at weight files (no writer boilerâ€‘plate) | ğŸ–¼ï¸        |
| 5  | **Remote prod inspection** â€“ `sshfs` into a containerized model            | ğŸ›°ï¸        |
| 6  | **CI sanity** â€“ fail build if any `*.grad` == 0 bytes ğŸ¤¨                   | ğŸ”¨         |
| 7  | **Observability** â€“ Grafana agent tails `/logs/fuse.log`                   | ğŸ“‰         |
| 8  | **Fuseâ€‘RPC inference** â€“ write input tensor, read output, no gRPC          | ğŸ“¡         |
| 9  | **Edutainment** â€“ students literally `cd` into ResNet tunnels              | ğŸ“         |
| 10 | **Security diff** â€“ hash all blobs, alarm on tamper                        | ğŸ›¡ï¸        |
| 11 | **Delta checkpoints** â€“ `rsync --inplace` on file blobs                    | ğŸ’¾         |
| 12 | **Lazy mmap serving** â€“ roadmap: weights stay on disk, pageâ€‘in live        | ğŸ›¸         |

---


## ğŸ”­ Roadmap â€” where MLFS can venture next ğŸš€

| Track | What it buys you | Effort estimate | Caveats / Gotchas |
|-------|------------------|-----------------|-------------------|
| **ONNX / TensorFlow / TFLite backâ€‘ends** | Mount *any* framework's weightsâ†’ same tree â†’ crossâ€‘tool diffing & hacks | **Medium** â€“ parse each format once, expose tensors as `memoryview` | ONNX & TF easy (protobuf blobs); TFLite flatbuffers parser needed |
| **GPUâ€‘mmap weights (GPUDirect Storage)** | Model bytes stream straight from file into GPU memory â€“ no CPU copy, instant warmâ€‘up | **Hard** â€“ integrate `cuFile` / `cudaMallocHost` pinned pages | Requires A100/H100â€‘class HW + kernel mods |
| **Lazy safetensors index** | Subâ€‘second mount of 20 GB Llama; chunkâ€‘SHA dedup across checkpoints | **Medium** â€“ we already mmap safetensors, add index & cache | â€” |
| **Writeâ€‘back plugin** | FS edits â†’ regenerate `.pt` / `.safetensors` automatically | **Easy** â€“ invert `build_tree`, embed dtype/shape in `/sys` | â€” |
| **Activation sharedâ€‘memory taps** | Another process can `mmap` live feature maps â†’ realâ€‘time viz | **Mediumâ€‘Hard** â€“ expose `/activations/*` as shm, need sync | Needs inotify/futex protocol |
| **FUSEâ€‘dmabuf zeroâ€‘copy tensors** | Kernel hands GPU a dmabuf handle â†’ true zeroâ€‘copy train | **Research** â€“ pending upstream FUSE patches | bleedingâ€‘edge kernel only |

> **Legend** â€“ *Effort* â‰ˆ weekend hack = Easy, few weekends = Medium, research rabbitâ€‘hole = Hard


---

## ğŸ§â€â™‚ï¸  License

MIT.  Go forth and mount exotically.

> "**Everything is a file**â€¦ even a 175â€‘billionâ€‘parameter fever dream." â€“ some POSIX wizard
